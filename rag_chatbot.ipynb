{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Criação de chat básico com RAG (Retrieval-Augmented Generation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install -U  openai  langchain  langchain_openai  langchain-community  python-dotenv  datasets  qdrant-client  tiktoken"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "from langchain_openai import AzureChatOpenAI\n",
    "\n",
    "load_dotenv('.env')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## AzureChatOpenAI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No tutorial foi usado a API da OpenAI diretamente e aqui foi usada a AzureOpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Criação do Chatbot \n",
    "\n",
    "Esse é um chat básico que consome o gpt 3.5 turbo 16k fornecido pela API da AzureChatOpenAI."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instânciamento do chat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "llm = AzureChatOpenAI(\n",
    "    azure_deployment=\"gpt-35-turbo-16k\",  \n",
    "    openai_api_version=\"2023-06-01-preview\",  \n",
    "    azure_endpoint=os.getenv(\"AZURE_OPENAI_ENDPOINT\"),\n",
    "    openai_api_key=os.getenv(\"AZURE_OPENAI_KEY\"), \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Do LangChain Schema são importados 3 elementos:\n",
    "- SystemMessage: Usado para definir escopo e tom do modelo;\n",
    "- HumanMessage: Define o conteúdo da mensagem humana;\n",
    "- AIMessage: Resposta do modelo.\n",
    "\n",
    "A lista messages é como um log do chat com o LLM. Todo o log é passado para o llm com o método invoke para questão de contexto."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Claro! Machine learning (aprendizado de máquina, em português) é um subcampo da inteligência artificial que se concentra no desenvolvimento de algoritmos e modelos de computador capazes de aprender e tomar decisões ou fazer previsões com base em dados sem serem explicitamente programados.\n",
      "\n",
      "A ideia central do machine learning é permitir que os computadores aprendam com exemplos e experiências passadas para melhorar seu desempenho em tarefas futuras. Os modelos de machine learning são treinados usando conjuntos de dados, nos quais são apresentados exemplos e uma função objetivo que descreve o que se deseja aprender ou prever.\n",
      "\n",
      "Existem diversas abordagens de machine learning, como aprendizado supervisionado, não supervisionado, por reforço, entre outras. No aprendizado supervisionado, o modelo aprende a partir de pares de entrada e saída rotulados, enquanto no aprendizado não supervisionado, o modelo busca por padrões e estrutura nos dados sem rótulos prévios.\n",
      "\n",
      "O machine learning tem aplicações em uma variedade de áreas, incluindo reconhecimento de padrões, reconhecimento de fala, processamento de linguagem natural, visão computacional, detecção de fraudes, previsão de mercado e muito mais. É uma área em rápido crescimento e com grande potencial para impulsionar a automação e a tomada de decisão baseada em dados.\n"
     ]
    }
   ],
   "source": [
    "from langchain.schema import (\n",
    "    SystemMessage,\n",
    "    HumanMessage,\n",
    "    AIMessage\n",
    ")\n",
    "## messages é a lista que corresponde ao Log da conversa\n",
    "messages = [\n",
    "    SystemMessage(content=\"Você é um assistente útil que responde perguntas.\"),\n",
    "    HumanMessage(content=\"Olá Bot, como você está hoje?\"),\n",
    "    AIMessage(content=\"Estou bem, obrigado. Como posso ajudar?\"),\n",
    "    HumanMessage(content=\"Gostaria de entender o que é machine learning.\")\n",
    "]\n",
    "\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando a resposta \"response\" ao log \"messages\". Isso é feito para manter o contexto da conversa."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Novo Prompt (HumanMessage) que é adicionado ao log (messages) através de append."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage (\n",
    "    content='Qual é a diferença entre aprendizado supervisionado e não supervisionado?'\n",
    ")\n",
    "messages.append(prompt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "No Aprendizado Supervisionado, os algoritmos são treinados usando pares de dados de entrada e a resposta esperada correspondente. Isso significa que o conjunto de dados de treinamento contém exemplos rotulados, onde cada entrada tem uma resposta conhecida associada. O objetivo do modelo de Machine Learning é aprender a mapear as entradas para as saídas corretas, de modo que possa fazer previsões ou classificar novos dados.\n",
      "\n",
      "Por exemplo, em um problema de classificação de e-mails em spam e não spam, o modelo de Aprendizado Supervisionado é treinado com e-mails rotulados como spam ou não spam. Com base nesses exemplos, o modelo aprende os padrões que distinguem os dois tipos de e-mails e pode classificar corretamente novos e-mails como spam ou não spam.\n",
      "\n",
      "Já no Aprendizado Não Supervisionado, não existem rótulos ou respostas esperadas nos conjuntos de dados de treinamento. Os algoritmos de Aprendizado Não Supervisionado exploram a estrutura interna dos dados para encontrar padrões, agrupamentos ou relações ocultas. Esses algoritmos são usados para descobrir informações valiosas sem nenhuma orientação supervisionada.\n",
      "\n",
      "Por exemplo, o Aprendizado Não Supervisionado pode ser usado para agrupar clientes em diferentes segmentos com base em seus comportamentos de compra em um conjunto de dados de transações. O algoritmo analisaria a estrutura interna dos dados e agruparia os clientes com perfis de compra semelhantes em diferentes clusters, semelhantes aos métodos de segmentação de mercado.\n",
      "\n",
      "Em resumo, a diferença chave entre o Aprendizado Supervisionado e o Não Supervisionado está na presença ou ausência de rótulos nos dados de treinamento, com o Aprendizado Supervisionado utilizando exemplos rotulados e o Aprendizado Não Supervisionado explorando os padrões internos dos dados.\n"
     ]
    }
   ],
   "source": [
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hallucination"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Teste 1, sem contexto do RAG:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response)\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content='O que tem de tão especial no Mistral 7B?'\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Desculpe, mas não estou familiarizado com uma referência específica ao \"Mistral 7B\". Pode ser um termo relacionado a algo específico como um produto, lugar ou uma designação que estou desconhecido. Poderia fornecer mais informações ou contexto para que eu possa entender melhor e ajudar?\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Teste 2, também sem contexto:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages.append(response)\n",
    "\n",
    "prompt = HumanMessage(\n",
    "    content='Você pode me falar sobre o LLMChain no LangChain?'\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Peço desculpas, mas não tenho informações sobre o \"LLMChain no LangChain\". Não tenho conhecimento de uma referência específica a esses termos. Poderia fornecer mais detalhes ou contexto para que eu possa ajudar de forma mais precisa? Estou aqui para responder suas perguntas da melhor maneira possível.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context injection\n",
    "\n",
    "Aqui o contexto é adicionado ao prompt manualmente. A fonte externa de dados são os dois parágrafos dentro da lista llmchain_information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "llmchain_information = [\n",
    "    \"A LLMChain is the most common type of chain. It consists of a PromptTemplate, a model (either an LLM or a ChatModel), and an optional output parser. This chain takes multiple input variables, uses the PromptTemplate to format them into a prompt. It then passes that to the model. Finally, it uses the OutputParser (if provided) to parse the output of the LLM into a final format.\",\n",
    "    \"Chains is an incredibly generic concept which returns to a sequence of modular components (or other chains) combined in a particular way to accomplish a common use case.\",\n",
    "    \"LangChain is a framework for developing applications powered by language models. We believe that the most powerful and differentiated applications will not only call out to a language model via an api, but will also: (1) Be data-aware: connect a language model to other sources of data, (2) Be agentic: Allow a language model to interact with its environment. As such, the LangChain framework is designed with the objective in mind to enable those types of applications.\"\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1020"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "source_knowledge = \"\\n\".join(llmchain_information)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Prompt enriquecido manualmente\n",
    "\n",
    "A query equivale ao input do usuário e o augmented_prompt é composto de uma espécie de System Message para o modelo, um contexto que é a fonte externa e a pergunta (query)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "query = 'Você pode me falar sobre o LLMChain no LangChain?'\n",
    "\n",
    "augmented_prompt = f\"\"\"Use o contexto abaixo para responder à pergunta\n",
    "\n",
    "Contexto:\n",
    "{source_knowledge}\n",
    "\n",
    "Pergunta: {query}\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Adicionando o augmented_prompt ao log de mensagens, passando o log ao modelo e exibindo o content da resposta."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O LLMChain no LangChain é um componente central e comum na estrutura do LangChain. É um tipo de cadeia (chain) que consiste em um PromptTemplate, um modelo (seja um LLM ou um ChatModel) e um analisador de saída opcional. Essa cadeia (chain) recebe várias variáveis de entrada, usa o PromptTemplate para formatá-las em uma estimulação e, em seguida, passa essa estimulação para o modelo. Por fim, usa o analisador de saída (se fornecido) para converter a saída do LLM (Linguagem de Modelo de Aprendizado) em um formato final.\n",
      "\n",
      "A estrutura das cadeias (chains) no LangChain é altamente modular e flexível, permitindo a combinação de componentes de forma específica para atender a diferentes casos de uso. O LangChain é um framework para desenvolver aplicações alimentadas por modelos de linguagem, buscando conectar o modelo de linguagem com várias fontes de dados e possibilitar a interação desse modelo com o ambiente em que opera.\n",
      "\n",
      "A combinação de cadeias de LLM e outros componentes no LangChain permite a criação de aplicações poderosas e diferenciadas que vão além de simplesmente chamar um modelo de linguagem por meio de uma API, proporcionando um ambiente data-aware e agentic para essa interação.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=augmented_prompt\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "response = llm.invoke(messages)\n",
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Embeddings "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "dataset usado - Mistral 7b on Hugging Face:  https://huggingface.co/datasets/infoslack/mistral-7b-arxiv-paper-chunked"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import AzureOpenAIEmbeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# O embedding não funciona caso não haja esse \"tratamento\" antes.\n",
    "if \"OPENAI_API_BASE\" in os.environ:\n",
    "    del os.environ[\"OPENAI_API_BASE\"]\n",
    "\n",
    "# Inicialize o AzureOpenAIEmbeddings\n",
    "embed_model = AzureOpenAIEmbeddings(\n",
    "    azure_endpoint=os.environ[\"AZURE_OPENAI_ENDPOINT\"],\n",
    "    azure_deployment='text-embedding-3-small-2',  # modelo de recuperação usado\n",
    "    openai_api_version=\"2023-05-15\",\n",
    "    api_key=os.environ[\"AZURE_OPENAI_KEY\"],\n",
    "    chunk_size=1000,  # Adicione o parâmetro chunk_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 159,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2, 1536)"
      ]
     },
     "execution_count": 159,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "texts = [\n",
    "    'this is one chunk',\n",
    "    'this is the second chunk of text'\n",
    "]\n",
    "\n",
    "res = embed_model.embed_documents(texts)\n",
    "len(res), len(res[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preparando fonte de conhecimento externa"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Carregamento dos dados"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import load_dataset\n",
    "dataset = load_dataset('infoslack/mistral-7b-arxiv-paper-chunked', split=\"train\")\n",
    "dataset[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Conversão do conjunto para um dataframe do Pandas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = dataset.to_pandas()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para o exemplo só precisaremos de duas colunas, chunk e source, então isolamos essas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 163,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk</th>\n",
       "      <th>source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>automated benchmarks. Our models are released ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>GQA significantly accelerates the inference sp...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Mistral 7B takes a significant step in balanci...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>parameters of the architecture are summarized ...</td>\n",
       "      <td>http://arxiv.org/pdf/2310.06825</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                               chunk                           source\n",
       "0  Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayr...  http://arxiv.org/pdf/2310.06825\n",
       "1  automated benchmarks. Our models are released ...  http://arxiv.org/pdf/2310.06825\n",
       "2  GQA significantly accelerates the inference sp...  http://arxiv.org/pdf/2310.06825\n",
       "3  Mistral 7B takes a significant step in balanci...  http://arxiv.org/pdf/2310.06825\n",
       "4  parameters of the architecture are summarized ...  http://arxiv.org/pdf/2310.06825"
      ]
     },
     "execution_count": 163,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs = data[['chunk', 'source']]\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usamos o método load da classe DataFrameLoader do pacote document_loaders para carregar os registos no formado de documento do LangChain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.document_loaders import DataFrameLoader\n",
    "\n",
    "loader = DataFrameLoader(docs, page_content_column='chunk')\n",
    "documents = loader.load()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Document(metadata={'source': 'http://arxiv.org/pdf/2310.06825'}, page_content='Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src')"
      ]
     },
     "execution_count": 169,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 166,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'"
      ]
     },
     "execution_count": 166,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].page_content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'source': 'http://arxiv.org/pdf/2310.06825'}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "documents[0].metadata"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Todos esses dados trabalhados são adicionados ao banco de dados vetorizado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Qdrant\n",
    "from langchain_openai import AzureOpenAIEmbeddings\n",
    "embeddings = AzureOpenAIEmbeddings(model='text-embedding-3-small-2')\n",
    "\n",
    "qdrant = Qdrant.from_documents(\n",
    "    documents=documents,\n",
    "    embedding=embeddings,\n",
    "    location=':memory:',\n",
    "    collection_name=\"chatbot\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Consulta a ser feita da base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [],
   "source": [
    "query= \"O que tem de tão especial no Mistral 7B?\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usa-se o método de busca de similaridade do qdrant para passar a query ao banco vetorizado e o parâmetro k define o limite de resposta que, no caso, são 3 chunks.<br>\n",
    "O retorno serão os 3 chunks mais similares ao que foi perguntado."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Mistral 7B\\nAlbert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\\nDevendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\\nGuillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\\nPierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\\nWilliam El Sayed\\nAbstract\\nWe introduce Mistral 7B, a 7–billion-parameter language model engineered for\\nsuperior performance and efficiency. Mistral 7B outperforms the best open 13B\\nmodel (Llama 2) across all evaluated benchmarks, and the best released 34B\\nmodel (Llama 1) in reasoning, mathematics, and code generation. Our model\\nleverages grouped-query attention (GQA) for faster inference, coupled with sliding\\nwindow attention (SWA) to effectively handle sequences of arbitrary length with a\\nreduced inference cost. We also provide a model fine-tuned to follow instructions,\\nMistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\\nautomated benchmarks. Our models are released under the Apache 2.0 license.\\nCode: https://github.com/mistralai/mistral-src'"
      ]
     },
     "execution_count": 174,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qdrant_search = qdrant.similarity_search(query, k=3)\n",
    "\n",
    "# Estou lendo o page_content do primeiro chunk da lista. O método similarity_search retorna 3 chunks dentro de uma lista e para visualizar a mensagem de cada é preciso acessar o índice correspondente.\n",
    "qdrant_search[0].page_content"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A função custom_prompt usa o resultado da busca por similaridade para fazer um prompt enriquecido onde é passado o comando, o contexto e a pergunta que estará na variável \"query\"."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 175,
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_prompt (query: str):\n",
    "    results = qdrant.similarity_search(query, k=3)\n",
    "    source_knowledge = \"\\n\".join([x.page_content for x in results])\n",
    "    augmented_prompt = f\"\"\"Use o contexto abaixo para responder à pergunta.\n",
    "\n",
    "    Contexto: \n",
    "    {source_knowledge}\n",
    "\n",
    "    Pergunta: {query}\"\"\"\n",
    "    return augmented_prompt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exemplo de como o prompt enriquecido ficará."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use o contexto abaixo para responder à pergunta.\n",
      "\n",
      "    Contexto: \n",
      "    Mistral 7B\n",
      "Albert Q. Jiang, Alexandre Sablayrolles, Arthur Mensch, Chris Bamford,\n",
      "Devendra Singh Chaplot, Diego de las Casas, Florian Bressand, Gianna Lengyel,\n",
      "Guillaume Lample, Lucile Saulnier, Lélio Renard Lavaud, Marie-Anne Lachaux,\n",
      "Pierre Stock, Teven Le Scao, Thibaut Lavril, Thomas Wang, Timothée Lacroix,\n",
      "William El Sayed\n",
      "Abstract\n",
      "We introduce Mistral 7B, a 7–billion-parameter language model engineered for\n",
      "superior performance and efficiency. Mistral 7B outperforms the best open 13B\n",
      "model (Llama 2) across all evaluated benchmarks, and the best released 34B\n",
      "model (Llama 1) in reasoning, mathematics, and code generation. Our model\n",
      "leverages grouped-query attention (GQA) for faster inference, coupled with sliding\n",
      "window attention (SWA) to effectively handle sequences of arbitrary length with a\n",
      "reduced inference cost. We also provide a model fine-tuned to follow instructions,\n",
      "Mistral 7B – Instruct, that surpasses Llama 2 13B – chat model both on human and\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "decoding, allowing for higher batch sizes hence higher throughput, a crucial factor for real-time\n",
      "applications. In addition, SWA is designed to handle longer sequences more effectively at a reduced\n",
      "computational cost, thereby alleviating a common limitation in LLMs. These attention mechanisms\n",
      "collectively contribute to the enhanced performance and efficiency of Mistral 7B.arXiv:2310.06825v1  [cs.CL]  10 Oct 2023\n",
      "Mistral 7B is released under the Apache 2.0 license. This release is accompanied by a reference\n",
      "implementation1facilitating easy deployment either locally or on cloud platforms such as AWS, GCP,\n",
      "or Azure using the vLLM [ 17] inference server and SkyPilot2. Integration with Hugging Face3is\n",
      "also streamlined for easier integration. Moreover, Mistral 7B is crafted for ease of fine-tuning across\n",
      "a myriad of tasks. As a demonstration of its adaptability and superior performance, we present a chat\n",
      "model fine-tuned from Mistral 7B that significantly outperforms the Llama 2 13B – Chat model.\n",
      "Mistral 7B takes a significant step in balancing the goals of getting high performance while keeping\n",
      "large language models efficient. Through our work, our aim is to help the community create more\n",
      "automated benchmarks. Our models are released under the Apache 2.0 license.\n",
      "Code: https://github.com/mistralai/mistral-src\n",
      "Webpage: https://mistral.ai/news/announcing-mistral-7b/\n",
      "1 Introduction\n",
      "In the rapidly evolving domain of Natural Language Processing (NLP), the race towards higher model\n",
      "performance often necessitates an escalation in model size. However, this scaling tends to increase\n",
      "computational costs and inference latency, thereby raising barriers to deployment in practical,\n",
      "real-world scenarios. In this context, the search for balanced models delivering both high-level\n",
      "performance and efficiency becomes critically essential. Our model, Mistral 7B, demonstrates that\n",
      "a carefully designed language model can deliver high performance while maintaining an efficient\n",
      "inference. Mistral 7B outperforms the previous best 13B model (Llama 2, [ 26]) across all tested\n",
      "benchmarks, and surpasses the best 34B model (LLaMa 34B, [ 25]) in mathematics and code\n",
      "generation. Furthermore, Mistral 7B approaches the coding performance of Code-Llama 7B [ 20],\n",
      "without sacrificing performance on non-code related benchmarks.\n",
      "Mistral 7B leverages grouped-query attention (GQA) [ 1], and sliding window attention (SWA) [ 6,3].\n",
      "GQA significantly accelerates the inference speed, and also reduces the memory requirement during\n",
      "\n",
      "    Pergunta: O que tem de tão especial no Mistral 7B?\n"
     ]
    }
   ],
   "source": [
    "print(custom_prompt(query))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 177,
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "\n",
    "response = llm.invoke(messages)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 178,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "O Mistral 7B é um modelo de linguagem com 7 bilhões de parâmetros que foi projetado para ter um desempenho superior e eficiência. Ele supera os melhores modelos abertos de 13B (Llama 2) em todos os benchmarks avaliados e o melhor modelo lançado de 34B (Llama 1) em áreas como raciocínio, matemática e geração de código. O modelo utiliza atenção agrupada por consulta (GQA) para acelerar o processo de inferência e atenção de janela deslizante (SWA) para lidar de forma eficaz com sequências de comprimento arbitrário com um custo de inferência reduzido. Além disso, também é fornecido um modelo ajustado para seguir instruções, o Mistral 7B - Instruir, que supera o modelo de chat Llama 2 13B em benchmarks humanos e automatizados. O Mistral 7B foi projetado para ser adaptável e facilmente ajustável a várias tarefas. Sua eficiência e desempenho aprimorados são alcançados por meio desses mecanismos de atenção. Ele é licenciado sob a Apache 2.0 e possui implementação de referência para facilitar o uso em nuvem e integração com outras ferramentas como a Hugging Face. O objetivo do Mistral 7B é equilibrar alto desempenho e eficiência em modelos de linguagem grandes.\n"
     ]
    }
   ],
   "source": [
    "print(response.content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Usando o modelo de \"unidade de processamento de linguagem\" (LPU) que gera respostas de IA extremamente rápido.<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instalação pip"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting langchain-groq\n",
      "  Downloading langchain_groq-0.2.4-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting groq<1,>=0.4.1 (from langchain-groq)\n",
      "  Downloading groq-0.18.0-py3-none-any.whl.metadata (14 kB)\n",
      "Requirement already satisfied: langchain-core<0.4.0,>=0.3.33 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langchain-groq) (0.3.40)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.8.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (0.28.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (2.10.6)\n",
      "Requirement already satisfied: sniffio in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (1.3.1)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.10 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from groq<1,>=0.4.1->langchain-groq) (4.12.2)\n",
      "Requirement already satisfied: langsmith<0.4,>=0.1.125 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (0.3.11)\n",
      "Requirement already satisfied: tenacity!=8.4.0,<10.0.0,>=8.1.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (9.0.0)\n",
      "Requirement already satisfied: jsonpatch<2.0,>=1.33 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (1.33)\n",
      "Requirement already satisfied: PyYAML>=5.3 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (6.0.2)\n",
      "Requirement already satisfied: packaging<25,>=23.2 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langchain-core<0.4.0,>=0.3.33->langchain-groq) (24.2)\n",
      "Requirement already satisfied: idna>=2.8 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from anyio<5,>=3.5.0->groq<1,>=0.4.1->langchain-groq) (3.10)\n",
      "Requirement already satisfied: certifi in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (2025.1.31)\n",
      "Requirement already satisfied: httpcore==1.* in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (1.0.7)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from httpcore==1.*->httpx<1,>=0.23.0->groq<1,>=0.4.1->langchain-groq) (0.14.0)\n",
      "Requirement already satisfied: jsonpointer>=1.9 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.33->langchain-groq) (3.0.0)\n",
      "Requirement already satisfied: orjson<4.0.0,>=3.9.14 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (3.10.15)\n",
      "Requirement already satisfied: requests<3,>=2 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (2.32.3)\n",
      "Requirement already satisfied: requests-toolbelt<2.0.0,>=1.0.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (1.0.0)\n",
      "Requirement already satisfied: zstandard<0.24.0,>=0.23.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (0.23.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.27.2 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from pydantic<3,>=1.9.0->groq<1,>=0.4.1->langchain-groq) (2.27.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (3.4.1)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in c:\\users\\gusta\\onedrive\\documentos\\github\\tutorial_rag\\test-rag\\lib\\site-packages (from requests<3,>=2->langsmith<0.4,>=0.1.125->langchain-core<0.4.0,>=0.3.33->langchain-groq) (2.3.0)\n",
      "Downloading langchain_groq-0.2.4-py3-none-any.whl (14 kB)\n",
      "Downloading groq-0.18.0-py3-none-any.whl (121 kB)\n",
      "Installing collected packages: groq, langchain-groq\n",
      "Successfully installed groq-0.18.0 langchain-groq-0.2.4\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.2 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "! pip install langchain-groq"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Importação da classe a partir do langchain_groq.<br>\n",
    "Para esse chat o modelo usado foi o DeepSeek r1 de 70 bilhões de parâmetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_groq import ChatGroq\n",
    "\n",
    "# if \"GROQ_API_KEY\" not in os.environ:\n",
    "#     os.environ[\"GROQ_API_KEY\"] = getpass.getpass(\"Enter your Groq API key: \")\n",
    "groq_api_key = os.environ[\"GROQ_API_KEY\"]\n",
    "\n",
    "chat = ChatGroq(temperature=0, model_name=\"deepseek-r1-distill-llama-70b\", )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<think>\n",
      "Ok, o usuário está perguntando o que tem de tão especial no Mistral 7B. Primeiro, preciso lembrar que o contexto fornecido inclui detalhes técnicos sobre o modelo. O usuário pode estar interessado em entender por que o Mistral 7B se destaca em relação a outros modelos, como o Llama 2 ou Llama 1.\n",
      "\n",
      "Vou começar analisando o contexto. Vejo que o Mistral 7B tem 7 bilhões de parâmetros e foi projetado para desempenho superior e eficiência. Isso já é um ponto importante, pois muitos modelos grandes sacrificam a eficiência pelo desempenho.\n",
      "\n",
      "Em seguida, noto que ele supera o Llama 2 13B em todos os benchmarks avaliados e o Llama 1 34B em raciocínio, matemática e geração de código. Isso mostra que, apesar de ser menor em tamanho, o Mistral 7B é mais eficiente e performático em certas áreas.\n",
      "\n",
      "Outro ponto-chave são as técnicas de atenção utilizadas: Grouped-Query Attention (GQA) e Sliding Window Attention (SWA). A GQA acelera a velocidade de inferência e reduz os requisitos de memória, permitindo lotes maiores e maior taxa de transferência, o que é crucial para aplicações em tempo real. Já a SWA lida melhor com sequências mais longas com menor custo computacional, resolvendo uma limitação comum em grandes modelos de linguagem.\n",
      "\n",
      "Além disso, o modelo é fine-tuned para seguir instruções, chamado Mistral 7B – Instruct, que supera o Llama 2 13B – Chat em benchmarks humanos e automatizados. Isso indica que ele é mais capaz de entender e executar instruções de forma eficaz.\n",
      "\n",
      "O fato de o Mistral 7B ser lançado sob a licença Apache 2.0 e acompanhado de uma implementação de referência facilita sua implantação local ou em nuvens como AWS, GCP ou Azure. Isso é importante para desenvolvedores e empresas que desejam usá-lo em seus projetos sem restrições de licença.\n",
      "\n",
      "Por fim, o equilíbrio entre desempenho e eficiência é algo notável. Muitos modelos grandes são poderosos, mas consomem muitos recursos. O Mistral 7B consegue manter um desempenho alto sem aumentar excessivamente os custos computacionais, o que o torna mais acessível e prático para uso real.\n",
      "\n",
      "Resumindo, os pontos especiais são: desempenho superior em benchmarks, eficiência com GQA e SWA, capacidade de lidar com sequências longas, fine-tuning para instruções, licença permissiva e facilidade de implantação. Essas características o tornam uma ferramenta poderosa e acessível no campo do processamento de linguagem natural.\n",
      "</think>\n",
      "\n",
      "O **Mistral 7B** é um modelo de linguagem de 7 bilhões de parâmetros projetado para oferecer desempenho superior e eficiência. Ele se destaca por várias características:\n",
      "\n",
      "1. **Desempenho Superior**:  \n",
      "   - Superou o modelo **Llama 2 13B** em todos os benchmarks avaliados.  \n",
      "   - Superou o modelo **Llama 1 34B** em tarefas de raciocínio, matemática e geração de código.  \n",
      "   - Aproximou o desempenho do **Code-Llama 7B** em geração de código, sem comprometer o desempenho em outras tarefas.\n",
      "\n",
      "2. **Técnicas de Atenção Inovadoras**:  \n",
      "   - Utiliza **Grouped-Query Attention (GQA)**, que acelera a velocidade de inferência e reduz os requisitos de memória, permitindo lotes maiores e maior taxa de transferência.  \n",
      "   - Utiliza **Sliding Window Attention (SWA)**, que lida melhor com sequências longas a um custo computacional reduzido, resolvendo uma limitação comum em grandes modelos de linguagem.\n",
      "\n",
      "3. **Fine-Tuning para Instruções**:  \n",
      "   - Oferece uma versão fine-tuned, **Mistral 7B – Instruct**, que supera o **Llama 2 13B – Chat** em benchmarks humanos e automatizados.\n",
      "\n",
      "4. **Eficiência e Praticidade**:  \n",
      "   - Projetado para equilibrar desempenho e eficiência, tornando-o mais acessível para aplicações práticas e em tempo real.  \n",
      "   - Lançado sob a licença **Apache 2.0**, com uma implementação de referência que facilita a implantação local ou em nuvens (AWS, GCP, Azure).  \n",
      "   - Integração simplificada com a plataforma **Hugging Face** para uso mais fácil.\n",
      "\n",
      "5. **Adaptabilidade**:  \n",
      "   - Foi projetado para ser facilmente fine-tuned para uma variedade de tarefas, demonstrando sua versatilidade e capacidade de adaptação.\n",
      "\n",
      "Em resumo, o **Mistral 7B** se destaca por seu desempenho superior, eficiência computacional, técnicas inovadoras de atenção e facilidade de implantação, tornando-o uma ferramenta poderosa e prática no campo do processamento de linguagem natural.\n"
     ]
    }
   ],
   "source": [
    "prompt = HumanMessage(\n",
    "    content=custom_prompt(query)\n",
    ")\n",
    "\n",
    "messages.append(prompt)\n",
    "response = chat.invoke(messages)\n",
    "print(response.content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "test-rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
